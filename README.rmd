---
title: MS-Ana
output: 
  github_document:
    toc: true
    toc_depth: 2
    pandoc_args: --webtex
---

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3699092.svg)](https://doi.org/10.5281/zenodo.3699092)

# Purpose

# Organisation 

The project is organized into the following folders:
```
|- analysis :   contains the R scripts 
|               (analysis.R, checkRep.R, quantify.R)
|               and their auxillary functions
|
|- data :       default folder for the input tables and 
|               data to be analyzed
|
|- results :    where the outputs of the scripts are stored
|  |
   |- figs :    figures
   |
   |- tables :  tables
```

The scripts should be run from the `analysis` folder,
and the paths to the required folders are defined in the 
scripts as:
```
# Define Data and Results repositories
dataRepo = '../data/'
figRepo  = '../results/figs/'
tabRepo  = '../results/tables/'
```
The MS and DMS files are expected by default to be in `data`.
For complex projects, they can be placed in sub-folders of `data`,
and their paths are given in the input files.

# Input files

Three input files (named `taskTable`, `tgTable` and `quantTable`)
are used by the set of scripts :

Script \\ Input|`taskTable`|`tgTable`|`quantTable`    
---------------|-----------|---------|----------    
`analysis.R`   |  X        |     X   |    
`checkRep.R`   |  X        |         |  X
`quantify.R`   |  X        |         |  X    


## `taskTable`  

This file defines the list of MS/DMS files to be analyzed.

It is a “comma” (,) delimited text file. 
It can be edited using excel or Rstudio (safer). 

`taskTable` is structured like this:

 MS_file | DMS_file | t0 | CV0 | dilu | Path
 --------|----------|----|-----|------|-----
C0_AS_DV-1800_1.d.ascii | Fichier_Dims 20190517-000000.txt | 0.08 | 6 | 0 | Esquire_MSMS_Data/2019_A_Voir/20190517_AA/

Where: 

 * `C0_AS_DV-1800_1.d.ascii` is an ASCII file, extracted using 
    DATAANALYSIS. So far, only the ESQUIRE data files extracted 
    using the `profile` option can be handled.
    It is stored in a sub-folder of `data` defined by `Path`.

 * `Fichier_Dims 20190517-000000.txt` is the corresponding 
   DMS file.
   It is expected to be in the `data` folder.


 * `t0` and `CV0` are used to convert the ESQUIRE 
   time _t_ values into DMS _CV_ values.

* `dilu`  was initially meant to be the dilution factor of the 
  standard metabolites when spiked into a plasma (see checkRep). 
  When you perform another type of experiment, you can use  
  `dilu` as an index to specify, for example, the flow-rate of 
  the modifier, the day of the experiment, the set of samples...
  
 * `Path` allows you to organize your data within the `../data/` 
 folder. Note that the DMS_files must be in the `../data` folder. 
 In the present example, only the MS_files are expected to be 
 found in the following folder: `../data/Esquire_MSMS_Data/2019_A_Voir/20190517_AA/`.

__Notes__

* lines starting with “# “ will be considered as comment lines

* the date from the DMS_file (here _20190517_)  
and the root of the MS file name (here `C0_AS_DV-1800_1`) 
are combined to tag the output files 
(_e.g._, `20190517_ C0_AS_DV-1800_1.results`)
This _tag_ in also used for the output figures.

## `tgTable`

This file contains the list of compounds to be analyzed in each
MS/DMS data set.

It  is a “semicolon” (;) __ TO BE MATCHED WITH TASKTABLE !!!__ delimited text file. 
It can be edited using excel or Rstudio (safer). 

`tgTable` is structued like this:

Name|m/z_EExact|m/z_exact|CV_ref
----|----------|---------|------
# Gly-AA|C2H5NO2H|76|-10.7
Ala-AA|90.054955|90.1|-7.6

Where: 

* `Name` is the given name of a metabolite,

* `m/z_EExact` is presently not used

* `m/z_exact` can actually be an approximate _m/z_ value

* `CV_ref` is the expected _CV_ value (can be omitted)

__Notes__

* lines starting with “# “ will be considered as comment lines.
  In the present example, Glycine will not be analyzed. 

## `quantTable`

TBD

# Scripts

## `analysis.R` 

(last version from 2020, July 16)

For each DMS-MS/MS experiment as given in a series in the 
`taskTable` file, the series of metabolites given in  `tgTable`
is analyzed. 
The aim of the analysis is to integrate the peak 
(_i.e._, to estimate the area) corresponding to each metabolite.

In the present version, a Gaussian peak shape is used.
The formula of a Gaussian function is
\[
G(x;a,x_0,\sigma)=\frac{a}{\sqrt{2\pi}\sigma} 
  \exp\left(-\frac{1}{2}\left(\frac{x-x_0}{\sigma}\right)^2\right)
\]
where $a$ is the area, $x_0$ is the position of the peak, 
and $\sigma$ is related to the full width at maximum ($fwhm$) by 
$fwhm = 2\sqrt{2\log(2)} \sigma$. 
Upon the fit process of the data, the area ($a$) is optimized, 
as well as the peak's position and width ($x_0$ and  $\sigma$). 

From the two dimensional data (_m/z_, _CV_), the area can be 
extracted using a 2D fit where the fit function is the product 
of two Gaussian functions, one in the _m/z_, the other in 
the _CV_ dimension. 

It turns out that we need three types of fit:

* 2D fit in the (_m/z_, _CV_) space

* 1D fit in the (_CV_) space, assuming that the _m/z_ value 
  is `m/z_exact` as given in `tgTable` 
  __(not exactly: the nearest peak position is used)__

* 1D fit in the _m/z_ space, assuming that the _CV_ value 
  is the `CV_ref` given in the `tgTable`


### Controle variables

The choice of fit type is set using the `fit_dim` variable. 
More generally, the important user configuration parameters 
are listed within the first line of the `analysis.R` script 
as follows:

```
# User configuration params ####
taskTable = 'files_quantification_2019.csv'
tgTable   = 'targets_paper.csv'

save_figures = TRUE
plot_maps    = FALSE

# Fit controling parameters
fit_dim  = 0    # in (0, 1, 2)
fallback = TRUE # Fallback on fit_dim=1 if 2D fit fails

weighted_fit  = FALSE
const_fwhm    = ifelse(fit_dim == 0,NA,0.7)

refine_CV0 = TRUE
dmz = 1.0       # Width of mz window around
                # exact mz for signal averaging
dCV = 1.2       # Width of CV window around
                # reference CV for peak fit

filter_results = TRUE
fwhm_mz_min = 0.1
fwhm_mz_max = 0.5
fwhm_cv_min = 0.5
fwhm_cv_max = 1.5
area_min    = 10

```

* `taskTable`: (string) file path to the tasks table 

* `tgTable`: (string) file path to the targets table 

* `save_figures`: (logical) save the plots on disk 

* `plot_maps`: (logical) generate 2D maps summarizing
   the position of fitted targets for a given task

* `fit_dim`: (interger) fit dimension and type:

    + `fit_dim = 2`: a two_dimensional (_m/z_,_CV_) fit is
       performed

    + `fit_dim = 1`: a 1D fit in the _CV_ dimension is performed.   
    + `fit_dim = 0`:  a 1D fit, but in the _m/z_ dimension 
       at fixed `CV_ref` (initially named “fast”).

* `fallback`:  (logical) use `fit_dim=1` in cases where 
  `fit_dim=2` fails (optimizer does not converge).

* `weighted_fit`: (logical) apply a Poisson-type weighting
   to the fitted data

* `const_fwhm`: (numerical) value of the peak's fwhm in the 
  _CV_ dimension (`fit_dim=1,2`) of the _m/z_ dimention 
  (`fit_dim=0`). If `const_fwhm=NA`, the value is optimized,
  otherwise, it is fixed to the specified value.

* `refine_CV0`: (logical) refine the center of the search window
  for the _CV_ position of the peak. 
  If `FALSE`, use the value defined in `tgTable`.

* `dmz`, `dCV`: (numericals) width of search intervals for the 
  peak's position. 
  These intervals are centered on (possibly refined) values 
  of `m/z_exact` and `CV_ref` given in `tgTable`.

* `filter_results`: (logical) filter the recovered peak widths 
  and areas. 
  The filtering rejects fwhm values outside of
  
    + [`fwhm_mz_min`,`fwhm_mz_max`] in the _m/z_ dimension
    
    + [`fwhm_cv_min`,`fwhm_cv_max`] in the _CV_ dimension
    
  and areas smaller than `area_min`.


### Outputs

The output files can be found in the following repositories:

```
figRepo  = '../results/figs/'
tabRepo  = '../results/tables/'
```

#### Figures 

For each task and target, you get a figure (on the screen and 
as a file) as shown below.

For a 2D fit: 
![](article/fig1.png)

For a 1D fit along m/z, i.e., `fit_dim=0`: 
![](article/fig2.png)

#### Tables 

For each experiments/task associated with (MS_file, DMS_file), 
three '.csv' files are generated.

If your data are (MS_file= C0_AS_DV-1800_1.d.ascii,
DMS_file= Fichier_Dims 20190517-000000.txt), 
and if `fit_dim=2`, you get:

* a results file named  `20190517_C0_AS_DV-1800_1_fit_dim_2_results.csv` 
with the following columns: 

    + the first 4 columns are copies of the `tgTable` data:    
    
      |Name|m/z_EExact|m/z_exact|CV_ref
      |----|----------|---------|------
    + the next 8 columns correspond to the position, width and
    uncertainty values of the optimized Gaussian in the m/z 
    and CV dimensions (unavailable data are represented by `NA`) 

      |m/z|u_m/z|CV|u_CV|FWHM_m/z|u_FWHM_m/z|FWHM_CV|u_FWHM_CV
      |---|-----|--|----|--------|----------|-------|---------
    + the next 2 columns are the results for the optimized Area
    values, and corresponding uncertainty.

      |Area|u_Area
      |----|------
    + finally, you will find the `fit_dim` value, the `dilu` index,
    and the `tag` which is a concatenation of date + MS_filename +
    fit_dim that can be used for further sorting of the results. 

      |fit_dim|dilu|tag
      |-------|----|---

## `checkRep.R`

This script collects the set of results files generated by
`analysis.R` as specified in the `taskTable` and generates
figures and statistics. 
The peak parameters are plotted as a function of `dilu`.
If `dilu` contains the experiment index, `checkRep.R` can be
used to assess the repeatability of an analysis.

### Controle variables

The job is defined by a few parameters.

````
taskTable  = 'files_quantification_2019July10.csv'
quantTable = 'targets_paper_quantification.csv'

fit_dim = 2
userTag = paste0('fit_dim_',fit_dim)

const_fwhm = 0.7

makePlots = TRUE
````

* `taskTable`: (string) file path to the list of experiments
  to be compared

* `quantTable`: (string) list of the compounds for which
  the comparisons should be done
  
* `fit_dim`: (integer) type of peak fit for which the comparisons
  should be done

* `userTag`: (string) tag to differentiate the outputs.
  In the present case, one wants to compare the repeatability
  for different peak fit approaches. 

* `const_fwhm`: (numerical) estimate of the peak width in
  the _CV_ direction to define the plot axes.
  
* `makePlots`: (logical) generate the plots

### Outputs

#### Figs

Presently, the plots are generated in the Rstudio interface,
but no saved to disk.

#### Tables

A ".csv" table containing all the collected results, with the following additions:

* two columns containing the ratio of areas for pairs
  of compounds defined in `quantTable`, and its uncertainty

  |ratio|u_ratio
  |-----|-------  
  
* a set of lines with tag "Mean", containing for each 
  target compound, the mean of the properties over the 
  set of experiments.
  Weighted means are estimated, based on the inverse of 
  the squared uncertainties.

The name of the file is a concatenation of the date, time,
`userTag`, and '_compilation.csv'

## `quantify.R`

This script is based on the same principle as `checkRep.R`
(same input files), but aims to estimate the quantification
parameters, such as the LOD.

### Controle variables

The job is defined by a few parameters.

````
taskTable  = 'files_quantification_2019July10.csv'
quantTable = 'targets_paper_quantification.csv'

fit_dim = 2
userTag = paste0('fit_dim_',fit_dim)

````

* `taskTable`: (string) file path to the list of experiments
  to be compared

* `quantTable`: (string) list of the compounds for which
  the comparisons should be done
  
* `fit_dim`: (integer) type of peak fit for which the comparisons
  should be done

* `userTag`: (string) tag to differentiate the outputs.
  In the present case, one wants to compare the repeatability
  for different peak fit approaches. 


### Outputs

#### Figs

A PDF file is generated, containing the quantification plots
for all compounds.

The name of the file is a concatenation of the date, time,
`userTag`, and '_quantif.pdf'

#### Tables

A ".csv" table containing the quantification results,
with columns

|Name|Int|Slo|Slo0|LOD
|----|---|---|----|---

where 

* 'Name' is the name of the compound

* 'Int' is the value of the intercept

* 'Slo' is the value of the slope

* 'Slo0' is the value of the slope with null intercept

* 'LOD' is the estimated limit of detection


The name of the file is a concatenation of the date, time,
`userTag`, and '_quantif.csv'
